{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:03.763576Z",
     "start_time": "2024-05-02T13:08:57.304648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "import os\n",
    "\n",
    "ollama.chat(model=\"llama3:instruct\", messages=[{\"role\":\"user\", \"content\":\"你是?请用中文回答。\"}])\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:instruct',\n",
       " 'created_at': '2024-05-02T13:09:03.76183695Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': '😊我是一个人工智能 chatbot，旨在帮助用户交流和解决问题。我可以理解自然语言，并根据输入提供相应的回复或答案。我并不是一个具体的人，但我会尽力为您服务！💬'},\n",
       " 'done': True,\n",
       " 'total_duration': 6454760934,\n",
       " 'load_duration': 962692712,\n",
       " 'prompt_eval_count': 18,\n",
       " 'prompt_eval_duration': 574997000,\n",
       " 'eval_count': 53,\n",
       " 'eval_duration': 4787304000}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:06.374836Z",
     "start_time": "2024-05-02T13:09:03.764053Z"
    }
   },
   "cell_type": "code",
   "source": "ollama.generate(model=\"llama3:instruct\", prompt=\"你好！\")",
   "id": "f765373e1b64831c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:instruct',\n",
       " 'created_at': '2024-05-02T13:09:06.324083672Z',\n",
       " 'response': '😊 你好！ Nice to meet you! 👋 How can I help or chat with you today? 😊',\n",
       " 'done': True,\n",
       " 'context': [128006,\n",
       "  882,\n",
       "  128007,\n",
       "  198,\n",
       "  198,\n",
       "  57668,\n",
       "  53901,\n",
       "  6447,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  76460,\n",
       "  232,\n",
       "  118195,\n",
       "  53901,\n",
       "  6447,\n",
       "  29959,\n",
       "  311,\n",
       "  3449,\n",
       "  499,\n",
       "  0,\n",
       "  62904,\n",
       "  233,\n",
       "  2650,\n",
       "  649,\n",
       "  358,\n",
       "  1520,\n",
       "  477,\n",
       "  6369,\n",
       "  449,\n",
       "  499,\n",
       "  3432,\n",
       "  30,\n",
       "  27623,\n",
       "  232,\n",
       "  128009],\n",
       " 'total_duration': 2559329056,\n",
       " 'load_duration': 939151,\n",
       " 'prompt_eval_count': 7,\n",
       " 'prompt_eval_duration': 251355000,\n",
       " 'eval_count': 25,\n",
       " 'eval_duration': 2176353000}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:06.383395Z",
     "start_time": "2024-05-02T13:09:06.376640Z"
    }
   },
   "cell_type": "code",
   "source": "ollama.list()",
   "id": "ba8f68c8b7985f58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'llama3:instruct',\n",
       "   'model': 'llama3:instruct',\n",
       "   'modified_at': '2024-05-02T08:24:27.013228038+08:00',\n",
       "   'size': 4661224578,\n",
       "   'digest': 'a6990ed6be412c6a217614b0ec8e9cd6800a743d5dd7e1d7fbe9df09e61d5615',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "35f47718c1b2f26d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 使用ollama客户端API连接ollama服务端的方式访问LLM",
   "id": "8d2586041fe7de0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:09.787589Z",
     "start_time": "2024-05-02T13:09:06.384554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host=\"http://127.0.0.1:11434\")\n",
    "response = client.chat(model=\"llama3:instruct\", messages=[\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"你好？\",\n",
    "    },\n",
    "])\n",
    "print(response)"
   ],
   "id": "bf5fe6492de34b54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3:instruct', 'created_at': '2024-05-02T13:09:09.786167298Z', 'message': {'role': 'assistant', 'content': '😊 你好！我是语言模型，我很高兴地看到您问候。请随时提出问题或开始对话，我们可以一起聊天！'}, 'done': True, 'total_duration': 3382802476, 'load_duration': 42324204, 'prompt_eval_count': 6, 'prompt_eval_duration': 223027000, 'eval_count': 35, 'eval_duration': 3070742000}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 客户端异步访问",
   "id": "50c2a77a55c5e42d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:40.200378Z",
     "start_time": "2024-05-02T13:09:40.190760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "    message = {\"role\": \"user\", \"content\": \"你好？\"}\n",
    "    async for part in await AsyncClient().chat(model=\"llama3:instruct\", messages=[message], stream=True):\n",
    "        print(part, flush=True)\n",
    "    \n",
    "asyncio.run(chat())\n"
   ],
   "id": "13c61d71e42c081d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingjing/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/pygments/regexopt.py:78: RuntimeWarning: coroutine 'chat' was never awaited\n",
      "  for group in groupby(strings, lambda s: s[0] == first[0])) \\\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m AsyncClient()\u001B[38;5;241m.\u001B[39mchat(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama3:instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m, messages\u001B[38;5;241m=\u001B[39m[message], stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(part, flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 9\u001B[0m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.12/asyncio/runners.py:190\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(main, debug, loop_factory)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03mThis function runs the passed coroutine, taking care of\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m    asyncio.run(main())\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;66;03m# fail fast with short traceback\u001B[39;00m\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Runner(debug\u001B[38;5;241m=\u001B[39mdebug, loop_factory\u001B[38;5;241m=\u001B[39mloop_factory) \u001B[38;5;28;01mas\u001B[39;00m runner:\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mrun(main)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:29:28.920970Z",
     "start_time": "2024-05-02T13:29:22.566783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import Client\n",
    "\n",
    "def get_completion(prompt):\n",
    "    client = Client(host=\"http://127.0.0.1:11434\")\n",
    "    resp = client.chat(model=\"llama3:instruct\", messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ])\n",
    "    return resp\n",
    "\n",
    "\n",
    "customer_email = ''' \\\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "'''\n",
    "\n",
    "style = ''' \\\n",
    "American English \\\n",
    "in a calm and respectful tone\n",
    "'''\n",
    "\n",
    "# 使用 'f' 字符串和说明来指定提示.\n",
    "# 把用三个反引号扩起来的文本翻译成\"style\"那样的风格。\n",
    "prompt = f'''\n",
    "Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "text： ```{customer_email}```\n",
    "'''\n",
    "\n",
    "# 输出完整的提示语\n",
    "print(prompt)\n",
    "\n",
    "resp = get_completion(prompt)\n",
    "\n",
    "print(resp['message']['content'])\n"
   ],
   "id": "5a41c5203a7b0868",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate the text that is delimited by triple backticks into a style that is  American English in a calm and respectful tone\n",
      ".\n",
      "text： ``` Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n",
      "Here is the translation:\n",
      "\n",
      "```I'm absolutely furious that my blender lid flew off and covered my kitchen walls in a mess of smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I really need your help right now.```\n",
      "\n",
      "Let me know if you'd like any further adjustments!\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:53:46.588656Z",
     "start_time": "2024-05-02T13:53:46.554526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import Client\n",
    "\n",
    "def get_completion(prompt):\n",
    "    client = Client(host=\"http://127.0.0.1:11434\")\n",
    "    resp = client.chat(model=\"llama3:instruct\", messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ])\n",
    "    return resp\n",
    "\n",
    "\n",
    "customer_email = ''' \\\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "'''\n",
    "\n",
    "style = ''' \\\n",
    "American English \\\n",
    "in a calm and respectful tone\n",
    "'''\n",
    "\n",
    "# 使用 'f' 字符串和说明来指定提示.\n",
    "# 把用三个反引号扩起来的文本翻译成\"style\"那样的风格。\n",
    "prompt = f'''\n",
    "Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "text： ```{customer_email}```\n",
    "'''\n",
    "\n",
    "# 输出完整的提示语\n",
    "#print(prompt)\n",
    "\n",
    "#resp = get_completion(prompt)\n",
    "\n",
    "#print(resp['message']['content'])\n",
    "\n",
    "# 以上通过llama3将不太礼貌的 `custom_email` 转变为更加礼貌性的表述。\n",
    "# 假设现在要针对不同的语言来进行转换，这需要生成一整套针对不同语言的提示来生成这样的翻译。\n",
    "# 接下来看看 LangChain 如何实现这一要求\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "chat = ChatOllama(temperature=0.0)\n",
    "\n",
    "# 为了反复使用上述模板，我们导入 LangChain 的 ChatPromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "customer_msg = prompt_template.format_messages(style=style, text=customer_email)\n",
    "print(type(customer_msg))\n",
    "print(type(customer_msg[0]))\n",
    "\n",
    "print(customer_msg[0])\n",
    "custer_resp = chat(customer_msg)\n",
    "\n",
    "print(custer_resp)\n",
    "# print(prompt_template)\n",
    "\n",
    "\n"
   ],
   "id": "96cd9f8e95d7bf4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "content=\"\\nTranslate the text that is delimited by triple backticks into a style that is  American English in a calm and respectful tone\\n.\\ntext： ``` Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\"\n"
     ]
    },
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOllamaEndpointNotFoundError\u001B[0m               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 62\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(customer_msg[\u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mprint\u001B[39m(customer_msg[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 62\u001B[0m custer_resp \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcustomer_msg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mprint\u001B[39m(custer_resp)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# print(prompt_template)\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     emit_warning()\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:729\u001B[0m, in \u001B[0;36mBaseChatModel.__call__\u001B[0;34m(self, messages, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    721\u001B[0m \u001B[38;5;129m@deprecated\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.1.7\u001B[39m\u001B[38;5;124m\"\u001B[39m, alternative\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minvoke\u001B[39m\u001B[38;5;124m\"\u001B[39m, removal\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    723\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    727\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    728\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m--> 729\u001B[0m     generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    730\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    731\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    732\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(generation, ChatGeneration):\n\u001B[1;32m    733\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m generation\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:407\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    406\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 407\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    408\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    409\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    411\u001B[0m ]\n\u001B[1;32m    412\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:397\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    395\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    396\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 397\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    403\u001B[0m         )\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:589\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    585\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    586\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    587\u001B[0m         )\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 589\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    593\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/chat_models/ollama.py:257\u001B[0m, in \u001B[0;36mChatOllama._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate\u001B[39m(\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    235\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    239\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[1;32m    240\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001B[39;00m\n\u001B[1;32m    241\u001B[0m \n\u001B[1;32m    242\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;124;03m            ])\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 257\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    264\u001B[0m     chat_generation \u001B[38;5;241m=\u001B[39m ChatGeneration(\n\u001B[1;32m    265\u001B[0m         message\u001B[38;5;241m=\u001B[39mAIMessage(content\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mtext),\n\u001B[1;32m    266\u001B[0m         generation_info\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mgeneration_info,\n\u001B[1;32m    267\u001B[0m     )\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatResult(generations\u001B[38;5;241m=\u001B[39m[chat_generation])\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/chat_models/ollama.py:188\u001B[0m, in \u001B[0;36mChatOllama._chat_stream_with_aggregation\u001B[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chat_stream_with_aggregation\u001B[39m(\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    181\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    186\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatGenerationChunk:\n\u001B[1;32m    187\u001B[0m     final_chunk: Optional[ChatGenerationChunk] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 188\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_chat_stream_response_to_chat_generation_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/chat_models/ollama.py:161\u001B[0m, in \u001B[0;36mChatOllama._create_chat_stream\u001B[0;34m(self, messages, stop, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_chat_stream\u001B[39m(\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    154\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m    155\u001B[0m     stop: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    157\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    158\u001B[0m     payload \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_messages_to_ollama_messages(messages),\n\u001B[1;32m    160\u001B[0m     }\n\u001B[0;32m--> 161\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_stream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpayload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapi_url\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_url\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/api/chat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/llms/ollama.py:246\u001B[0m, in \u001B[0;36m_OllamaCommon._create_stream\u001B[0;34m(self, api_url, payload, stop, **kwargs)\u001B[0m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m404\u001B[39m:\n\u001B[0;32m--> 246\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m OllamaEndpointNotFoundError(\n\u001B[1;32m    247\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOllama call failed with status code 404. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    248\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaybe your model is not found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    249\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand you should pull the model with `ollama pull \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    250\u001B[0m         )\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    252\u001B[0m         optional_detail \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mtext\n",
      "\u001B[0;31mOllamaEndpointNotFoundError\u001B[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`."
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
