{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:03.763576Z",
     "start_time": "2024-05-02T13:08:57.304648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "import os\n",
    "\n",
    "ollama.chat(model=\"llama3:instruct\", messages=[{\"role\":\"user\", \"content\":\"ä½ æ˜¯?è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚\"}])\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:instruct',\n",
       " 'created_at': '2024-05-02T13:09:03.76183695Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'ğŸ˜Šæˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½ chatbotï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·äº¤æµå’Œè§£å†³é—®é¢˜ã€‚æˆ‘å¯ä»¥ç†è§£è‡ªç„¶è¯­è¨€ï¼Œå¹¶æ ¹æ®è¾“å…¥æä¾›ç›¸åº”çš„å›å¤æˆ–ç­”æ¡ˆã€‚æˆ‘å¹¶ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„äººï¼Œä½†æˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æœåŠ¡ï¼ğŸ’¬'},\n",
       " 'done': True,\n",
       " 'total_duration': 6454760934,\n",
       " 'load_duration': 962692712,\n",
       " 'prompt_eval_count': 18,\n",
       " 'prompt_eval_duration': 574997000,\n",
       " 'eval_count': 53,\n",
       " 'eval_duration': 4787304000}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:06.374836Z",
     "start_time": "2024-05-02T13:09:03.764053Z"
    }
   },
   "cell_type": "code",
   "source": "ollama.generate(model=\"llama3:instruct\", prompt=\"ä½ å¥½ï¼\")",
   "id": "f765373e1b64831c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:instruct',\n",
       " 'created_at': '2024-05-02T13:09:06.324083672Z',\n",
       " 'response': 'ğŸ˜Š ä½ å¥½ï¼ Nice to meet you! ğŸ‘‹ How can I help or chat with you today? ğŸ˜Š',\n",
       " 'done': True,\n",
       " 'context': [128006,\n",
       "  882,\n",
       "  128007,\n",
       "  198,\n",
       "  198,\n",
       "  57668,\n",
       "  53901,\n",
       "  6447,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  76460,\n",
       "  232,\n",
       "  118195,\n",
       "  53901,\n",
       "  6447,\n",
       "  29959,\n",
       "  311,\n",
       "  3449,\n",
       "  499,\n",
       "  0,\n",
       "  62904,\n",
       "  233,\n",
       "  2650,\n",
       "  649,\n",
       "  358,\n",
       "  1520,\n",
       "  477,\n",
       "  6369,\n",
       "  449,\n",
       "  499,\n",
       "  3432,\n",
       "  30,\n",
       "  27623,\n",
       "  232,\n",
       "  128009],\n",
       " 'total_duration': 2559329056,\n",
       " 'load_duration': 939151,\n",
       " 'prompt_eval_count': 7,\n",
       " 'prompt_eval_duration': 251355000,\n",
       " 'eval_count': 25,\n",
       " 'eval_duration': 2176353000}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:06.383395Z",
     "start_time": "2024-05-02T13:09:06.376640Z"
    }
   },
   "cell_type": "code",
   "source": "ollama.list()",
   "id": "ba8f68c8b7985f58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'llama3:instruct',\n",
       "   'model': 'llama3:instruct',\n",
       "   'modified_at': '2024-05-02T08:24:27.013228038+08:00',\n",
       "   'size': 4661224578,\n",
       "   'digest': 'a6990ed6be412c6a217614b0ec8e9cd6800a743d5dd7e1d7fbe9df09e61d5615',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "35f47718c1b2f26d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ä½¿ç”¨ollamaå®¢æˆ·ç«¯APIè¿æ¥ollamaæœåŠ¡ç«¯çš„æ–¹å¼è®¿é—®LLM",
   "id": "8d2586041fe7de0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:09.787589Z",
     "start_time": "2024-05-02T13:09:06.384554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host=\"http://127.0.0.1:11434\")\n",
    "response = client.chat(model=\"llama3:instruct\", messages=[\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"ä½ å¥½ï¼Ÿ\",\n",
    "    },\n",
    "])\n",
    "print(response)"
   ],
   "id": "bf5fe6492de34b54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3:instruct', 'created_at': '2024-05-02T13:09:09.786167298Z', 'message': {'role': 'assistant', 'content': 'ğŸ˜Š ä½ å¥½ï¼æˆ‘æ˜¯è¯­è¨€æ¨¡å‹ï¼Œæˆ‘å¾ˆé«˜å…´åœ°çœ‹åˆ°æ‚¨é—®å€™ã€‚è¯·éšæ—¶æå‡ºé—®é¢˜æˆ–å¼€å§‹å¯¹è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€èµ·èŠå¤©ï¼'}, 'done': True, 'total_duration': 3382802476, 'load_duration': 42324204, 'prompt_eval_count': 6, 'prompt_eval_duration': 223027000, 'eval_count': 35, 'eval_duration': 3070742000}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# å®¢æˆ·ç«¯å¼‚æ­¥è®¿é—®",
   "id": "50c2a77a55c5e42d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:09:40.200378Z",
     "start_time": "2024-05-02T13:09:40.190760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "    message = {\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Ÿ\"}\n",
    "    async for part in await AsyncClient().chat(model=\"llama3:instruct\", messages=[message], stream=True):\n",
    "        print(part, flush=True)\n",
    "    \n",
    "asyncio.run(chat())\n"
   ],
   "id": "13c61d71e42c081d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingjing/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/pygments/regexopt.py:78: RuntimeWarning: coroutine 'chat' was never awaited\n",
      "  for group in groupby(strings, lambda s: s[0] == first[0])) \\\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m AsyncClient()\u001B[38;5;241m.\u001B[39mchat(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama3:instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m, messages\u001B[38;5;241m=\u001B[39m[message], stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(part, flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 9\u001B[0m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.12/asyncio/runners.py:190\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(main, debug, loop_factory)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03mThis function runs the passed coroutine, taking care of\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m    asyncio.run(main())\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;66;03m# fail fast with short traceback\u001B[39;00m\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Runner(debug\u001B[38;5;241m=\u001B[39mdebug, loop_factory\u001B[38;5;241m=\u001B[39mloop_factory) \u001B[38;5;28;01mas\u001B[39;00m runner:\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mrun(main)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:29:28.920970Z",
     "start_time": "2024-05-02T13:29:22.566783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import Client\n",
    "\n",
    "def get_completion(prompt):\n",
    "    client = Client(host=\"http://127.0.0.1:11434\")\n",
    "    resp = client.chat(model=\"llama3:instruct\", messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ])\n",
    "    return resp\n",
    "\n",
    "\n",
    "customer_email = ''' \\\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "'''\n",
    "\n",
    "style = ''' \\\n",
    "American English \\\n",
    "in a calm and respectful tone\n",
    "'''\n",
    "\n",
    "# ä½¿ç”¨ 'f' å­—ç¬¦ä¸²å’Œè¯´æ˜æ¥æŒ‡å®šæç¤º.\n",
    "# æŠŠç”¨ä¸‰ä¸ªåå¼•å·æ‰©èµ·æ¥çš„æ–‡æœ¬ç¿»è¯‘æˆ\"style\"é‚£æ ·çš„é£æ ¼ã€‚\n",
    "prompt = f'''\n",
    "Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "textï¼š ```{customer_email}```\n",
    "'''\n",
    "\n",
    "# è¾“å‡ºå®Œæ•´çš„æç¤ºè¯­\n",
    "print(prompt)\n",
    "\n",
    "resp = get_completion(prompt)\n",
    "\n",
    "print(resp['message']['content'])\n"
   ],
   "id": "5a41c5203a7b0868",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate the text that is delimited by triple backticks into a style that is  American English in a calm and respectful tone\n",
      ".\n",
      "textï¼š ``` Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n",
      "Here is the translation:\n",
      "\n",
      "```I'm absolutely furious that my blender lid flew off and covered my kitchen walls in a mess of smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I really need your help right now.```\n",
      "\n",
      "Let me know if you'd like any further adjustments!\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T13:53:46.588656Z",
     "start_time": "2024-05-02T13:53:46.554526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ollama import Client\n",
    "\n",
    "def get_completion(prompt):\n",
    "    client = Client(host=\"http://127.0.0.1:11434\")\n",
    "    resp = client.chat(model=\"llama3:instruct\", messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ])\n",
    "    return resp\n",
    "\n",
    "\n",
    "customer_email = ''' \\\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "'''\n",
    "\n",
    "style = ''' \\\n",
    "American English \\\n",
    "in a calm and respectful tone\n",
    "'''\n",
    "\n",
    "# ä½¿ç”¨ 'f' å­—ç¬¦ä¸²å’Œè¯´æ˜æ¥æŒ‡å®šæç¤º.\n",
    "# æŠŠç”¨ä¸‰ä¸ªåå¼•å·æ‰©èµ·æ¥çš„æ–‡æœ¬ç¿»è¯‘æˆ\"style\"é‚£æ ·çš„é£æ ¼ã€‚\n",
    "prompt = f'''\n",
    "Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "textï¼š ```{customer_email}```\n",
    "'''\n",
    "\n",
    "# è¾“å‡ºå®Œæ•´çš„æç¤ºè¯­\n",
    "#print(prompt)\n",
    "\n",
    "#resp = get_completion(prompt)\n",
    "\n",
    "#print(resp['message']['content'])\n",
    "\n",
    "# ä»¥ä¸Šé€šè¿‡llama3å°†ä¸å¤ªç¤¼è²Œçš„ `custom_email` è½¬å˜ä¸ºæ›´åŠ ç¤¼è²Œæ€§çš„è¡¨è¿°ã€‚\n",
    "# å‡è®¾ç°åœ¨è¦é’ˆå¯¹ä¸åŒçš„è¯­è¨€æ¥è¿›è¡Œè½¬æ¢ï¼Œè¿™éœ€è¦ç”Ÿæˆä¸€æ•´å¥—é’ˆå¯¹ä¸åŒè¯­è¨€çš„æç¤ºæ¥ç”Ÿæˆè¿™æ ·çš„ç¿»è¯‘ã€‚\n",
    "# æ¥ä¸‹æ¥çœ‹çœ‹ LangChain å¦‚ä½•å®ç°è¿™ä¸€è¦æ±‚\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "chat = ChatOllama(temperature=0.0)\n",
    "\n",
    "# ä¸ºäº†åå¤ä½¿ç”¨ä¸Šè¿°æ¨¡æ¿ï¼Œæˆ‘ä»¬å¯¼å…¥ LangChain çš„ ChatPromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "customer_msg = prompt_template.format_messages(style=style, text=customer_email)\n",
    "print(type(customer_msg))\n",
    "print(type(customer_msg[0]))\n",
    "\n",
    "print(customer_msg[0])\n",
    "custer_resp = chat(customer_msg)\n",
    "\n",
    "print(custer_resp)\n",
    "# print(prompt_template)\n",
    "\n",
    "\n"
   ],
   "id": "96cd9f8e95d7bf4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "content=\"\\nTranslate the text that is delimited by triple backticks into a style that is  American English in a calm and respectful tone\\n.\\ntextï¼š ``` Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\"\n"
     ]
    },
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOllamaEndpointNotFoundError\u001B[0m               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 62\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(customer_msg[\u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mprint\u001B[39m(customer_msg[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 62\u001B[0m custer_resp \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcustomer_msg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mprint\u001B[39m(custer_resp)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# print(prompt_template)\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     emit_warning()\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:729\u001B[0m, in \u001B[0;36mBaseChatModel.__call__\u001B[0;34m(self, messages, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    721\u001B[0m \u001B[38;5;129m@deprecated\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.1.7\u001B[39m\u001B[38;5;124m\"\u001B[39m, alternative\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minvoke\u001B[39m\u001B[38;5;124m\"\u001B[39m, removal\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    723\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    727\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    728\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m--> 729\u001B[0m     generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    730\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    731\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    732\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(generation, ChatGeneration):\n\u001B[1;32m    733\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m generation\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:407\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    406\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 407\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    408\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    409\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    411\u001B[0m ]\n\u001B[1;32m    412\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:397\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    395\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    396\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 397\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    403\u001B[0m         )\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:589\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    585\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    586\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    587\u001B[0m         )\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 589\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    593\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/chat_models/ollama.py:257\u001B[0m, in \u001B[0;36mChatOllama._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate\u001B[39m(\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    235\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    239\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[1;32m    240\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001B[39;00m\n\u001B[1;32m    241\u001B[0m \n\u001B[1;32m    242\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;124;03m            ])\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 257\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    264\u001B[0m     chat_generation \u001B[38;5;241m=\u001B[39m ChatGeneration(\n\u001B[1;32m    265\u001B[0m         message\u001B[38;5;241m=\u001B[39mAIMessage(content\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mtext),\n\u001B[1;32m    266\u001B[0m         generation_info\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mgeneration_info,\n\u001B[1;32m    267\u001B[0m     )\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatResult(generations\u001B[38;5;241m=\u001B[39m[chat_generation])\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/chat_models/ollama.py:188\u001B[0m, in \u001B[0;36mChatOllama._chat_stream_with_aggregation\u001B[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chat_stream_with_aggregation\u001B[39m(\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    181\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    186\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatGenerationChunk:\n\u001B[1;32m    187\u001B[0m     final_chunk: Optional[ChatGenerationChunk] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 188\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_chat_stream_response_to_chat_generation_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/chat_models/ollama.py:161\u001B[0m, in \u001B[0;36mChatOllama._create_chat_stream\u001B[0;34m(self, messages, stop, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_chat_stream\u001B[39m(\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    154\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m    155\u001B[0m     stop: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    157\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    158\u001B[0m     payload \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_messages_to_ollama_messages(messages),\n\u001B[1;32m    160\u001B[0m     }\n\u001B[0;32m--> 161\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_stream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpayload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapi_url\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_url\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/api/chat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/assistant-PcWefG4u-py3.12/lib/python3.12/site-packages/langchain_community/llms/ollama.py:246\u001B[0m, in \u001B[0;36m_OllamaCommon._create_stream\u001B[0;34m(self, api_url, payload, stop, **kwargs)\u001B[0m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m404\u001B[39m:\n\u001B[0;32m--> 246\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m OllamaEndpointNotFoundError(\n\u001B[1;32m    247\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOllama call failed with status code 404. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    248\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaybe your model is not found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    249\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand you should pull the model with `ollama pull \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    250\u001B[0m         )\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    252\u001B[0m         optional_detail \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mtext\n",
      "\u001B[0;31mOllamaEndpointNotFoundError\u001B[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`."
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
